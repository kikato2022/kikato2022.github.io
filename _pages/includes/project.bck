# ðŸ§° Project
## ðŸ“‚ Datasets

<!--=======Audio-Visual-Datasets======-->

[<a href="https://kikato.github.io/projects/vividex.html" target="_blank" rel="noopener">project</a>]
<div class='paper-box'><div class='paper-box-image'><div><img src='images/lrs3.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**LRS3-For-Speech-Separation**

**Kai Li**

[**Github Repo**](https://github.com/JusperLee/LRS3-For-Speech-Separation) \| <a href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/LRS3-For-Speech-Separation?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a>

- Open source audio-visual dataset processing script. Following are the steps to generate training and testing data. There are several parameters to change in order to match different purpose.
</div>
</div>

## ðŸŽ¤ Audio-only Speech Separation Methods
<!--=======dprnn======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/dprnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**DPRNN-Pytorch**

[**Github Repo**](https://github.com/JusperLee/Dual-Path-RNN-Pytorch) \| <a href="https://github.com/JusperLee/Dual-Path-RNN-Pytorch/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Dual-Path-RNN-Pytorch?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: DPRNNé˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/104606356)

**Kai Li**

- Dual-path RNN. Efficient long sequence modeling for time-domain single-channel speech separation implemented by Pytorch.
</div>
</div>

<!--=======sdr======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/sdr.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Calculate-SNR-SDR**

[**Github Repo**](https://github.com/JusperLee/Calculate-SNR-SDR) \| <a href="https://github.com/JusperLee/Calculate-SNR-SDR/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Calculate-SNR-SDR?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a>

**Kai Li**

- Calculatie Audioâ€˜s SNR and SDR.
</div>
</div>

<!--=======convtasnet======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/convtasnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Conv-TasNet**

[**Github Repo**](https://github.com/JusperLee/Conv-TasNet) \| <a href="https://github.com/JusperLee/Conv-TasNet/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Conv-TasNet?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: Conv-TasNeté˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/101235440)

**Kai Li**

- Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation Pytorch's Implement.
</div>
</div>

<!--=======upit======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/upit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**UtterancePIT**

[**Github Repo**](https://github.com/JusperLee/UtterancePIT-Speech-Separation) \| <a href="https://github.com/JusperLee/UtterancePIT-Speech-Separation/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/UtterancePIT-Speech-Separation?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: uPITé˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/101232793)

**Kai Li**

- According to funcwj's uPIT, the training code supporting multi-gpu is written, and the Dataloader is reconstructed.
</div>
</div>

<!--=======dpcl======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/dpcl.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Deep Clustering**

[**Github Repo**](https://github.com/JusperLee/Deep-Clustering-for-Speech-Separation) \| <a href="https://github.com/JusperLee/Deep-Clustering-for-Speech-Separation/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Deep-Clustering-for-Speech-Separation?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: DPCLé˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/101234149)

**Kai Li**

- Deep clustering in the field of speech separation implemented by pytorch.
</div>
</div>

<!--=======afrcnn======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/afrcnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AFRCNN**

[**Github Repo**](https://github.com/JusperLee/AFRCNN-For-Speech-Separation) \| <a href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/AFRCNN-For-Speech-Separation?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: AFRCNNé˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/101234149)

**Kai Li**

- Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network.
</div>
</div>

## ðŸŽ¬ Audio-visual Speech Separation Methods
<!--=======Look======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/look.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Looking to Listen at the Cocktail Party**

[**Github Repo**](https://github.com/JusperLee/Looking-to-Listen-at-the-Cocktail-Party) \| <a href="https://github.com/JusperLee/Looking-to-Listen-at-the-Cocktail-Party/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Looking-to-Listen-at-the-Cocktail-Party?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a> \| [**çŸ¥ä¹Ž: DPRNNé˜…è¯»ç¬”è®°**](https://zhuanlan.zhihu.com/p/86922308)

**Kai Li**

- The project is an audiovisual model reproduced by the contents of the paper Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation.
</div>
</div>

## ðŸ“– Tutorial
<!--=======papers======-->
<div class='paper-box'><div class='paper-box-image'><div><img src='images/tutorials.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Speech-Separation-Paper-Tutorial**

[**Github Repo**](https://github.com/JusperLee/Speech-Separation-Paper-Tutorial) \| <a href="https://github.com/JusperLee/Speech-Separation-Paper-Tutorial/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/JusperLee/Speech-Separation-Paper-Tutorial?color=green&style=flat-square" style="border-radius: 0; margin: 0; display: inherit;"></a>

**Kai Li**

- A must-read paper and tutorial list for speech separation based on neural networks.
</div>
</div>
